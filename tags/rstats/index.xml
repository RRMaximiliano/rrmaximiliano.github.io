<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats on Julia Silge</title>
    <link>/tags/rstats/</link>
    <description>Recent content in rstats on Julia Silge</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/rstats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LASSO regression using tidymodels and #TidyTuesday data for The Office</title>
      <link>/blog/lasso-the-office/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/lasso-the-office/</guid>
      <description>I‚Äôve been publishing screencasts demonstrating how to use the tidymodels framework, from first steps in modeling to how to tune more complex models. Today, I‚Äôm using this week‚Äôs #TidyTuesday dataset on The Office to show how to build a LASSO regression model and choose regularization parameters!
  
Here is the code I used in the video, for those who prefer reading instead of or in addition to video.</description>
    </item>
    
    <item>
      <title>Preprocessing and resampling using #TidyTuesday college data</title>
      <link>/blog/tuition-resampling/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/tuition-resampling/</guid>
      <description>I‚Äôve been publishing screencasts demonstrating how to use the tidymodels framework, from first getting started to how to tune machine learning models. Today, I‚Äôm using this week‚Äôs #TidyTuesday dataset on college tuition and diversity at US colleges to show some data preprocessing steps and how to use resampling!
  
Here is the code I used in the video, for those who prefer reading instead of or in addition to video.</description>
    </item>
    
    <item>
      <title>Hyperparameter tuning and #TidyTuesday food consumption</title>
      <link>/blog/food-hyperparameter-tune/</link>
      <pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/food-hyperparameter-tune/</guid>
      <description>Last week I published a screencast demonstrating how to use the tidymodels framework and specifically the recipes package. Today, I‚Äôm using this week‚Äôs #TidyTuesday dataset on food consumption around the world to show hyperparameter tuning!
  
Here is the code I used in the video, for those who prefer reading instead of or in addition to video.
Explore the data Our modeling goal here is to predict which countries are Asian countries and which countries are not, based on their patterns of food consumption in the eleven categories from the #TidyTuesday dataset.</description>
    </item>
    
    <item>
      <title>#TidyTuesday hotel bookings and recipes</title>
      <link>/blog/hotels-recipes/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/hotels-recipes/</guid>
      <description>Last week I published my first screencast showing how to use the tidymodels framework for machine learning and modeling in R. Today, I‚Äôm using this week‚Äôs #TidyTuesday dataset on hotel bookings to show how to use one of the tidymodels packages recipes with some simple models!
  
Here is the code I used in the video, for those who prefer reading instead of or in addition to video.</description>
    </item>
    
    <item>
      <title>#TidyTuesday and tidymodels</title>
      <link>/blog/intro-tidymodels/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/intro-tidymodels/</guid>
      <description>This week I started my new job as a software engineer at RStudio, working with Max Kuhn and other folks on tidymodels. I am really excited about tidymodels because my own experience as a practicing data scientist has shown me some of the areas for growth that still exist in open source software when it comes to modeling and machine learning. Almost nothing has had the kind of dramatic impact on my productivity that the tidyverse and other RStudio investments have had; I am enthusiastic about contributing to that kind of user-focused transformation for modeling and machine learning.</description>
    </item>
    
    <item>
      <title>Modeling salary and gender in the tech industry</title>
      <link>/blog/salary-gender/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/salary-gender/</guid>
      <description>One of the biggest projects I have worked on over the past several years is the Stack Overflow Developer Survey, and one of the most unique aspects of this survey is the extensive salary data that is collected. This salary data is used to power the Stack Overflow Salary Calculator, and has been used by various folks to explore how people who use spaces make more than those who use tabs, whether that‚Äôs just a proxy for open source contributions, and more.</description>
    </item>
    
    <item>
      <title>Opioid prescribing habits in Texas</title>
      <link>/blog/texas-opioids/</link>
      <pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/texas-opioids/</guid>
      <description>A paper I worked on was just published in a medical journal. This is quite an odd thing for me to be able to say, given my academic background and the career path I have had, but there you go! The first author of this paper is a long-time friend of mine working in anesthesiology and pain management, and he obtained data from the Texas Prescription Drug Monitoring Program (PDMP) about controlled substance prescriptions from April 2015 to 2018.</description>
    </item>
    
    <item>
      <title>(Re)Launching my supervised machine learning course</title>
      <link>/blog/supervised-ml-course/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/supervised-ml-course/</guid>
      <description>Today I am happy to announce a new(-ish), free, online, interactive course that I have developed, Supervised Machine Learning: Case Studies in R! üí´
Supervised machine learning in R Predictive modeling, or supervised machine learning, is a powerful tool for using data to make predictions about the world around us. Once you understand the basic ideas of supervised machine learning, the next step is to practice your skills so you know how to apply these techniques wisely and appropriately.</description>
    </item>
    
    <item>
      <title>Practice using lubridate... THEATRICALLY</title>
      <link>/blog/lubridate-london-stage/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/lubridate-london-stage/</guid>
      <description>I am so pleased to now be an RStudio-certified tidyverse trainer! üéâ I have been teaching technical content for decades, whether in a university classroom, developing online courses, or leading workshops, but I still found this program valuable for my own professonal development. I learned a lot that is going to make my teaching better, and I am happy to have been a participant. If you are looking for someone to lead trainings or workshops in your organization, you can check out this list of trainers to see who might be conveniently located to you!</description>
    </item>
    
    <item>
      <title>Introducing tidylo</title>
      <link>/blog/introducing-tidylo/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/introducing-tidylo/</guid>
      <description>Today I am so pleased to introduce a new package for calculating weighted log odds ratios, tidylo.
Often in data analysis, we want to measure how the usage or frequency of some feature, such as words, differs across some group or set, such as documents. One statistic often used to find these kinds of differences in text data is tf-idf. Another option is to use the log odds ratio, but the log odds ratio alone does not account for sampling variability.</description>
    </item>
    
    <item>
      <title>Reordering and facetting for ggplot2</title>
      <link>/blog/reorder-within/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/reorder-within/</guid>
      <description>I recently wrote about the release of tidytext 0.2.1, and one of the most useful new features in this release is a couple of helper functions for making plots with ggplot2. These helper functions address a class of challenges that often arises when dealing with text data, so we‚Äôve included them in the tidytext package.
Let‚Äôs work through an example To show how to use these new functions, let‚Äôs walk through a more general example that does not deal with results that come from unstructured, free text.</description>
    </item>
    
    <item>
      <title>Fixing your mistakes: sentiment analysis edition</title>
      <link>/blog/sentiment-lexicons/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/sentiment-lexicons/</guid>
      <description>Today tidytext 0.2.1 is available on CRAN! This new release of tidytext has a collection of nice new features.
 Bug squashing! üêõ Improvements to error messages and documentation üìÉ Switching from broom to generics for lighter dependencies Addition of some helper plotting functions I look forward to blogging about soon  An additional change is significant and may be felt by you, the user, so I want to share a bit about it.</description>
    </item>
    
    <item>
      <title>Relaunching the qualtRics package</title>
      <link>/blog/qualtrics-relaunch/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/qualtrics-relaunch/</guid>
      <description>Note: cross-posted with the rOpenSci blog.
rOpenSci is one of the first organizations in the R community I ever interacted with, when I participated in the 2016 rOpenSci unconf. I have since reviewed several rOpenSci packages and been so happy to be connected to this community, but I have never submitted or maintained a package myself. All that changed when I heard the call for a new maintainer for the qualtRics package.</description>
    </item>
    
    <item>
      <title>Writing a letter to DataCamp</title>
      <link>/blog/datacamp-misconduct/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/datacamp-misconduct/</guid>
      <description>Since 2017 I have been an instructor for DataCamp, the VC-backed online data science education platform. What this means is that I am not an employee, but I have developed content for the company as a contractor. I have two courses there, one on text mining and one on practical supervised machine learning.
About two weeks ago, DataCamp published a blog post outlining an incident of sexual misconduct at the company.</description>
    </item>
    
    <item>
      <title>Read all about it! Navigating the R Package Universe</title>
      <link>/blog/r-journal-navigating/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/r-journal-navigating/</guid>
      <description>In the most recent issue of the R Journal, I have a new paper out with coauthors John Nash and Spencer Graves. Check out the abstract:
 Today, the enormous number of contributed packages available to R users outstrips any given user‚Äôs ability to understand how these packages work, their relative merits, or how they are related to each other. We organized a plenary session at useR!2017 in Brussels for the R community to think through these issues and ways forward.</description>
    </item>
    
    <item>
      <title>Feeling the rstudio::conf ‚ù§Ô∏è</title>
      <link>/blog/rstudio-conf-2019/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/rstudio-conf-2019/</guid>
      <description>I am heading home from my third year of attending rstudio::conf! If you weren‚Äôt there, watch for the videos to be released so you can check out the talks; I know I will do the same so I can see the talks I was forced to miss by scheduling constraints. I love this conference, and once again this year, the organizers have succeeded in building an impactful, valuable, inclusive conference.</description>
    </item>
    
    <item>
      <title>Text classification with tidy data principles</title>
      <link>/blog/tidy-text-classification/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidy-text-classification/</guid>
      <description>I am an enthusiastic proponent of using tidy data principles for dealing with text data. This kind of approach offers a fluent and flexible option not just for exploratory data analysis, but also for machine learning for text, including both unsupervised machine learning and supervised machine learning. I haven‚Äôt written much about supervised machine learning for text, i.e.¬†predictive modeling, using tidy data principles, so let‚Äôs walk through an example workflow for this a text classification task.</description>
    </item>
    
    <item>
      <title>Word associations from the Small World of Words</title>
      <link>/blog/word-associations/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/word-associations/</guid>
      <description>Do you subscribe to the Data is Plural newsletter from Jeremy Singer-Vine? You probably should, because it is a treasure trove of interesting datasets arriving in your email inbox. In the November 28 edition, Jeremy linked to the Small World of Words project, and I was entranced. I love stuff like that, all about words and how people think of them. I have been mulling around a blog post ever since, and today I finally have my post done, so let‚Äôs see what‚Äôs up!</description>
    </item>
    
    <item>
      <title>TensorFlow, Jane Austen, and Text Generation</title>
      <link>/blog/tensorflow-generation/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/tensorflow-generation/</guid>
      <description>I remember the first time I saw a deep learning text generation project that was truly compelling and delightful to me. It was in 2016 when Andy Herd generated new Friends scenes by training a recurrent neural network on all the show‚Äôs episodes. Herd‚Äôs work went pretty viral at the time and I thought:
  via GIPHY And also:
  via GIPHY At the time I dabbled a bit with Andrej Karpathy‚Äôs tutorials for character-level RNNs; his work and tutorials undergird a lot of the kind of STUNT TEXT GENERATION work we see in the world.</description>
    </item>
    
    <item>
      <title>Training, evaluating, and interpreting topic models</title>
      <link>/blog/evaluating-stm/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/evaluating-stm/</guid>
      <description>At the beginning of this year, I wrote a blog post about how to get started with the stm and tidytext packages for topic modeling. I have been doing more topic modeling in various projects, so I wanted to share some workflows I have found useful for
 training many topic models at one time, evaluating topic models and understanding model diagnostics, and exploring and interpreting the content of topic models.</description>
    </item>
    
    <item>
      <title>Amazon Alexa and Accented English</title>
      <link>/blog/amazon-alexa/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/amazon-alexa/</guid>
      <description>Earlier this spring, one of my data science friends here in SLC got in contact with me about some fun analysis. My friend Dylan Zwick is a founder at Pulse Labs, a voice-testing startup, and they were chatting with the Washington Post about a piece on how devices like Amazon Alexa deal with accented English. The piece is published today in the Washington Post and turned out really interesting! Let‚Äôs walk through the analysis I did for Dylan and Pulse Labs.</description>
    </item>
    
    <item>
      <title>Punctuation in literature</title>
      <link>/blog/punctution-literature/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/punctution-literature/</guid>
      <description>This morning I was scrolling through Twitter and noticed Alberto Cairo share this lovely data visualization piece by Adam J. Calhoun about the varying prevalence of punctuation in literature. I thought, ‚ÄúI want to do that!‚Äù It also offers me the opportunity to chat about a few of the new options available for tokenizing in tidytext via updates to the tokenizers package.
Adam‚Äôs original piece explores how punctuation is used in nine novels, including my favorite Pride and Prejudice.</description>
    </item>
    
    <item>
      <title>Public Data Release of Stack Overflow‚Äôs 2018 Developer Survey</title>
      <link>/blog/stack-survey-2018/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/stack-survey-2018/</guid>
      <description>Note: Cross-posted with the Stack Overflow blog.
Starting today, you can access the public data release for Stack Overflow&amp;rsquo;s 2018 Developer Survey. Over 100,000 developers from around the world shared their opinions about everything from their favorite technologies to job preferences, and this data is now available for you to analyze yourself. This year, we are partnering with Kaggle to publish and highlight this dataset. This means you can access the data both here on our site and on Kaggle Datasets, and that on Kaggle, you can explore the dataset using Kernels.</description>
    </item>
    
    <item>
      <title>Understanding PCA using Stack Overflow data</title>
      <link>/blog/stack-overflow-pca/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/stack-overflow-pca/</guid>
      <description>This year, I have given some talks about understanding principal component analysis using what I spend day in and day out with, Stack Overflow data. You can see a recording of one of these talks from rstudio::conf 2018. When I have given these talks, I‚Äôve focused a lot on understanding PCA. This blog post walks through how I implemented PCA and how I made the plots I used in my talk.</description>
    </item>
    
    <item>
      <title>Stack Overflow questions around the world</title>
      <link>/blog/stack-questions-cities/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/stack-questions-cities/</guid>
      <description>I am so lucky to work with so many generous, knowledgeable, and amazing people at Stack Overflow, including Ian Allen and Kirti Thorat. Both Ian and Kirti are part of biweekly sessions we have at Stack Overflow where several software developers join me in practicing R, data science, and modeling skills. This morning, the two of them went to a high school outreach event in NYC for students who have been studying computer science, equipped with Stack Overflow ‚ú® SWAG ‚ú®, some coding activities based on Stack Overflow internal tools and packages, and a Shiny app that I developed to share a bit about who we are and what we do.</description>
    </item>
    
    <item>
      <title>The game is afoot! Topic modeling of Sherlock Holmes stories</title>
      <link>/blog/sherlock-holmes-stm/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/sherlock-holmes-stm/</guid>
      <description>In a recent release of tidytext, we added tidiers and support for building Structural Topic Models from the stm package. This is my current favorite implementation of topic modeling in R, so let‚Äôs walk through an example of how to get started with this kind of modeling, using The Adventures of Sherlock Holmes.
  via GIPHY You can watch along as I demonstrate how to start with the raw text of these short stories, prepare the data, and then implement topic modeling in this video tutorial!</description>
    </item>
    
    <item>
      <title>tidytext 0.1.6</title>
      <link>/blog/tidytext-0-1-6/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidytext-0-1-6/</guid>
      <description>I am pleased to announce that tidytext 0.1.6 is now on CRAN!
Most of this release, as well as the 0.1.5 release which I did not blog about, was for maintenance, updates to align with API changes from tidytext‚Äôs dependencies, and bugs. I just spent a good chunk of effort getting tidytext to pass R CMD check on older versions of R despite the fact that some of the packages in tidytext‚Äôs Suggests require recent versions of R.</description>
    </item>
    
    <item>
      <title>Tidy word vectors, take 2!</title>
      <link>/blog/word-vectors-take-two/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/word-vectors-take-two/</guid>
      <description>A few weeks ago, I wrote a post about finding word vectors using tidy data principles, based on an approach outlined by Chris Moody on the StitchFix tech blog. I‚Äôve been pondering how to improve this approach, and whether it would be nice to wrap up some of these functions in a package, so here is an update!
Like in my previous post, let‚Äôs download half a million posts from the Hacker News corpus using the bigrquery package.</description>
    </item>
    
    <item>
      <title>New sports from random emoji</title>
      <link>/blog/emoji-sports/</link>
      <pubDate>Sat, 25 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/emoji-sports/</guid>
      <description>I love emoji ‚ù§Ô∏è and I love xkcd, so this recent comic from Randall Munroe was quite a delight for me.
 I sat there, enjoying the thought of these new sports like horse hole and multiplayer avocado and I thought, ‚ÄúI can make more of these in just the barest handful of lines of code‚Äù. This is largely thanks to the emo package by Hadley Wickham, which if you haven‚Äôt installed and started using yet, WHY NOT?</description>
    </item>
    
    <item>
      <title>Word Vectors with tidy data principles</title>
      <link>/blog/tidy-word-vectors/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidy-word-vectors/</guid>
      <description>Last week I saw Chris Moody‚Äôs post on the Stitch Fix blog about calculating word vectors from a corpus of text using word counts and matrix factorization, and I was so excited! This blog post illustrates how to implement that approach to find word vector representations in R using tidy data principles and sparse matrices.
Word vectors, or word embeddings, are typically calculated using neural networks; that is what word2vec is.</description>
    </item>
    
    <item>
      <title>From Power Calculations to P-Values: A/B Testing at Stack Overflow</title>
      <link>/blog/ab-testing/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/ab-testing/</guid>
      <description>Note: cross-posted with the Stack Overflow blog.
If you hang out on Meta Stack Overflow, you may have noticed news from time to time about A/B tests of various features here at Stack Overflow. We use A/B testing to compare a new version to a baseline for a design, a machine learning model, or practically any feature of what we do here at Stack Overflow; these tests are part of our decision-making process.</description>
    </item>
    
    <item>
      <title>Mapping ecosystems of software development</title>
      <link>/blog/tag-network/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/tag-network/</guid>
      <description>I have a new post on the Stack Overflow blog today about the complex, interrelated ecosystems of software development. On the data team at Stack Overflow, we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other. One way to get at this idea of relationships between technologies is tag correlations, how often technology tags at Stack Overflow appear together relative to how often they appear separately.</description>
    </item>
    
    <item>
      <title>tidytext 0.1.4</title>
      <link>/blog/tidytext-0-1-4/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidytext-0-1-4/</guid>
      <description>I am pleased to announce that tidytext 0.1.4 is now on CRAN!
This release of our package for text mining using tidy data principles has an excellent collection of delightfulness in it. First off, all the important functions in tidytext now support support non-standard evaluation through the tidyeval framework.
library(janeaustenr) library(tidytext) library(dplyr) input_var &amp;lt;- quo(text) output_var &amp;lt;- quo(word) data_frame(text = prideprejudice) %&amp;gt;% unnest_tokens(!! output_var, !! input_var) ## # A tibble: 122,204 x 1 ## word ## &amp;lt;chr&amp;gt; ## 1 pride ## 2 and ## 3 prejudice ## 4 by ## 5 jane ## 6 austen ## 7 chapter ## 8 1 ## 9 it ## 10 is ## # .</description>
    </item>
    
    <item>
      <title>Sentiment analysis using tidy data principles at DataCamp</title>
      <link>/blog/sentiment-datacamp/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/sentiment-datacamp/</guid>
      <description>NOTE: Read more here about why I no longer recommend taking my courses at DataCamp. I‚Äôve been developing a course at DataCamp over the past several months, and I am happy to announce that it is now launched!
The course is Sentiment Analysis in R: the Tidy Way and I am excited that it is now available for you to explore and learn from. This course focuses on digging into the emotional and opinion content of text using sentiment analysis, and it does this from the specific perspective of using tools built for handling tidy data.</description>
    </item>
    
    <item>
      <title>Understanding gender roles in movies with text mining</title>
      <link>/blog/women-in-film/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/women-in-film/</guid>
      <description>I have a new visual essay up at The Pudding today, using text mining to explore how women are portrayed in film.
 The R code behind this analysis in publicly available on GitHub.
 I was so glad to work with the talented Russell Goldenberg and Amber Thomas on this project, and many thanks to Matt Daniels for inviting me to contribute to The Pudding. I‚Äôve been a big fan of their work for a long time!</description>
    </item>
    
    <item>
      <title>Seeking guidance in choosing and evaluating R packages</title>
      <link>/blog/package-guidance/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/package-guidance/</guid>
      <description>At useR!2017 in Brussels last month, I contributed to an organized session focused on navigating the 11,000+ packages on CRAN. My collaborators on this session and I recently put together an overall summary of the session and our goals, and now I‚Äôd like to talk more about the specific issue of learning about R packages and deciding which ones to use. John and Spencer will write more soon about the two other issues of our focus:</description>
    </item>
    
    <item>
      <title>Navigating the R Package Universe</title>
      <link>/blog/navigating-packages/</link>
      <pubDate>Wed, 26 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/navigating-packages/</guid>
      <description>Earlier this month, I, along with John Nash, Spencer Graves, and Ludovic Vannoorenberghe, organized a session at useR!2017 focused on discovering, learning about, and evaluating R packages. You can check out the recording of the session.
 There are more than 11,000 packages on CRAN, and R users must approach this abundance of packages with effective strategies to find what they need and choose which packages to invest time in learning how to use.</description>
    </item>
    
    <item>
      <title>Text Mining of Stack Overflow Questions</title>
      <link>/blog/text-mining-stack-overflow/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/text-mining-stack-overflow/</guid>
      <description>Note: Cross-posted with the Stack Overflow blog.
This week, my fellow Stack Overflow data scientist David Robinson and I are happy to announce the publication of our book Text Mining with R with O&amp;rsquo;Reilly. We are so excited to see this project out in the world, and so relieved to finally be finished with it! Text data is being generated all the time around us, in healthcare, finance, tech, and beyond; text mining allows us to transform that unstructured text data into real insight that can increase understanding and inform decision-making.</description>
    </item>
    
    <item>
      <title>Using tidycensus and leaflet to map Census data</title>
      <link>/blog/using-tidycensus/</link>
      <pubDate>Sat, 24 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/using-tidycensus/</guid>
      <description>Recently, I have been following the development and release of Kyle Walker‚Äôs tidycensus package. I have been filled with amazement, delight, and well, perhaps another feeling‚Ä¶
There should be a word for ‚Äúthe regret felt when an R üì¶, which would have saved untold hours of your life, is released‚Äù‚Ä¶ #rstats ü§î https://t.co/2THN4MwedO ‚Äî Mara Averick (@dataandme) May 31, 2017   But seriously, I have worked with US Census data a lot in the past and this package</description>
    </item>
    
    <item>
      <title>tidytext 0.1.3</title>
      <link>/blog/tidytext-0-1-3/</link>
      <pubDate>Sun, 18 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidytext-0-1-3/</guid>
      <description>I am pleased to announce that tidytext 0.1.3 is now on CRAN!
In this release, my collaborator David Robinson and I have fixed a handful of bugs, added tidiers for LDA models from the mallet package, and updated functions for changes to quanteda‚Äôs API. You can check out the NEWS for more details on changes.
One enhancement in this release is the addition of the Loughran and McDonald sentiment lexicon of words specific to financial reporting.</description>
    </item>
    
    <item>
      <title>Mining CRAN DESCRIPTION Files</title>
      <link>/blog/mining-cran-description/</link>
      <pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/mining-cran-description/</guid>
      <description>A couple of weeks ago, I saw on Dirk Eddelbuettel&amp;rsquo;s blog that R 3.4.0 was going to include a function for obtaining information about packages currently on CRAN, including basically everything in DESCRIPTION files. When R 3.4.0 was released, this was one of the things I was most immediately excited about exploring, because although I recently dabbled in scraping CRAN to try to get this kind of information, it was rather onerous.</description>
    </item>
    
    <item>
      <title>Gender Roles with Text Mining and N-grams</title>
      <link>/blog/gender-pronouns/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/gender-pronouns/</guid>
      <description>Today is the one year anniversary of the janeaustenr package‚Äôs appearance on CRAN, its cranniversary, if you will. I think it‚Äôs time for more Jane Austen here on my blog.
  via GIPHY I saw this paper by Matthew Jockers and Gabi Kirilloff a number of months ago and the ideas in it have been knocking around in my head ever since. The authors of that paper used text mining to examine a corpus of 19th century novels and explore how gendered pronouns (he/she/him/her) are associated with different verbs.</description>
    </item>
    
    <item>
      <title>How Do You Discover R Packages?</title>
      <link>/blog/package-search/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/package-search/</guid>
      <description>Like I mentioned in my last blog post, I am contributing to a session at userR 2017 this coming July that will focus on discovering and learning about R packages. This is an increasingly important issue for R users as we all decide which of the 10,000+ packages to invest time in understanding and then use in our work.
library(dplyr) available.packages() %&amp;gt;% tbl_df() ## # A tibble: 10,276 √ó 17 ## Package Version Priority Depends ## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; ## 1 A3 1.</description>
    </item>
    
    <item>
      <title>Scraping CRAN with rvest</title>
      <link>/blog/scraping-cran/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/scraping-cran/</guid>
      <description>I am one of the organizers for a session at userR 2017 this coming July that will focus on discovering and learning about R packages. How do R users find packages that meet their needs? Can we make this process easier? As somebody who is relatively new to the R world compared to many, this is a topic that resonates with me and I am happy to be part of the discussion.</description>
    </item>
    
    <item>
      <title>What Programming Languages Are Used Most on Weekends?</title>
      <link>/blog/weekends-weekdays/</link>
      <pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/weekends-weekdays/</guid>
      <description>Note: Cross-posted with the Stack Overflow blog. Check out the code for this analysis on Kaggle.
For me, the weekends are mostly about spending time with my family, reading for leisure, and working on the open-source projects I am involved in. These weekend projects overlap with the work that I do in my day job here at Stack Overflow, but are not exactly the same. Many developers tinker with side projects for learning or career development (or just for fun!</description>
    </item>
    
    <item>
      <title>Women in the 2016 Stack Overflow Survey</title>
      <link>/blog/women-survey/</link>
      <pubDate>Thu, 19 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/women-survey/</guid>
      <description>Note: Cross-posted with the Stack Overflow blog
The 2017 Stack Overflow Developer Survey opened last week, and we on the Data Team are looking forward to analyzing the survey results to better understand our developer community. I am particularly interested in women in tech, for probably obvious reasons, and recently I explored last year&amp;rsquo;s survey data to see what we can learn about women developers.
How many women took the developer survey?</description>
    </item>
    
    <item>
      <title>Text Mining in R: A Tidy Approach</title>
      <link>/blog/rstudio-conf/</link>
      <pubDate>Sat, 14 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/rstudio-conf/</guid>
      <description>I spoke on approaching text mining tasks using tidy data principles at rstudio::conf yesterday. I was so happy to have the opportunity to speak and the conference has been a great experience.
If you want to catch up on what has been going on at rstudio::conf, Karl Broman put together a GitHub repo of slides and Sharon Machlis has been live-blogging the conference at Computerworld. A highlight for me was Andrew Flowers&amp;rsquo; talk on data journalism and storytelling; I don&amp;rsquo;t work in data journalism but I think I can apply almost everything he said to how I approach what I do.</description>
    </item>
    
    <item>
      <title>Reddit Responds to the Election</title>
      <link>/blog/reddit-responds/</link>
      <pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/reddit-responds/</guid>
      <description>It&amp;rsquo;s been about a month since the U.S. presidential election, with Donald Trump&amp;rsquo;s victory over Hillary Clinton coming as a surprise to most. Reddit user Jason Baumgartner collected and published every submission and comment posted to Reddit on the day of (and a bit surrounding) the U.S. election; let&amp;rsquo;s explore this data set and see what kinds of things we can learn.
Data wrangling This first bit was the hardest part of this analysis for me, probably because I am not the most experienced JSON person out there.</description>
    </item>
    
    <item>
      <title>Measuring Gobbledygook</title>
      <link>/blog/gobbledygook/</link>
      <pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/gobbledygook/</guid>
      <description>In learning more about text mining over the past several months, one aspect of text that I‚Äôve been interested in is readability. A text‚Äôs readability measures how hard or easy it is for a reader to read and understand what a text is saying; it depends on how sentences are written, what words are chosen, and so forth. I first became really aware of readability scores of books through my kids‚Äô reading tracking websites for school, but it turns out there are lots of frameworks for measuring readability.</description>
    </item>
    
    <item>
      <title>Mapping Election Results in Utah</title>
      <link>/blog/election-mapping/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/election-mapping/</guid>
      <description>My adopted home state of Utah has been a weird place this election cycle. For the unfamiliar, Utah is extremely conservative when it comes to politics; it is one of the reddest of the red states and has backed the Republican candidate for president for the past many decades. In 2012, about 3/4 of the popular vote went to Mitt Romney (who is LDS, like many here in the state) and there were no counties where Mitt Romney did not win.</description>
    </item>
    
    <item>
      <title>Tidy Text Mining with R</title>
      <link>/blog/tidy-text-mining-with-r/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/tidy-text-mining-with-r/</guid>
      <description>I am so pleased to announce that tidytext 0.1.2 is now available on CRAN. This release of tidytext, a package for text mining using tidy data principles by Dave Robinson and me, includes some bug fixes and performance improvements, as well as some new functionality.
There is now a handy function for accessing the various lexicons in the sentiments dataset without the columns that are not used in that particular dataset; this makes these datasets even easier to use with pipes and joins from dplyr.</description>
    </item>
    
    <item>
      <title>Singing the Bayesian Beginner Blues</title>
      <link>/blog/bayesian-blues/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/bayesian-blues/</guid>
      <description>Earlier this week, I published a post about song lyrics and how different U.S. states are mentioned at different rates, and at different rates relative to their populations. That was a very fun post to work on, but you can tell from that paragraph near the end that I am a little bothered by the uncertainty involved in calculating the rates by just dividing two numbers. David Robinson suggested on Twitter that I might try using empirical Bayes methods to estimate the rates.</description>
    </item>
    
    <item>
      <title>Song Lyrics Across the United States</title>
      <link>/blog/song-lyrics-across/</link>
      <pubDate>Mon, 26 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/song-lyrics-across/</guid>
      <description>The inspiration for this post is a joint venture by both me and my husband, and its genesis lies more than 15 years in our past. One of the recurring conversations we have in our relationship (all long-term relationships have these, right?!) is about song lyrics and place names. I think the first time we ever had this conversation was in the late 1990s and was about Baltimore. &amp;ldquo;Why do so many songs talk about Baltimore?</description>
    </item>
    
    <item>
      <title>We Are Not Very Evenly Distributed</title>
      <link>/blog/evenly-distributed/</link>
      <pubDate>Fri, 19 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/evenly-distributed/</guid>
      <description>I saw this tweet making the rounds this past week.
Interesting! I saw people using this map to make the argument that the Electoral College was super important, or a terrible idea, or any of a number of other sociopolitical thoughts. This map certainly caught my attention and made me want to know more about this kind of population density distribution.
Census Population Data I use Census data from the American Community Survey a lot for my work, so let&amp;rsquo;s get the ACS population estimates for all the counties in the United States.</description>
    </item>
    
    <item>
      <title>Something Strange in the Neighborhood</title>
      <link>/blog/something-strange/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/something-strange/</guid>
      <description>Today I was so pleased to see a new data package hit CRAN, and how wonderful to see such accomplished women writing R packages.
What a great new data package on CRAN! And always great to see more women authors in #rstats https://t.co/nROMibqPxX pic.twitter.com/UEayWgx9bz ‚Äî Julia Silge (@juliasilge) August 5, 2016   The ghostr package includes a dataset of over 800 ghost sightings in Kentucky, with information on city, latitude, and longitude, along with URLs for finding more information about the ghost sightings.</description>
    </item>
    
    <item>
      <title>Return of the NEISS Data</title>
      <link>/blog/return-of-neiss/</link>
      <pubDate>Fri, 22 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/return-of-neiss/</guid>
      <description>Almost six months ago (!) I wrote a blog post about the NEISS data set, a sample of accidents reported to emergency rooms in the U.S. that are related to consumer products. Ever since I did that exploration, I have been wanting to ask a bit of a different question from that sample of accidents. How do the accidents that people suffer depend on their demographic characteristics? We can get a bit of a sense of that from looking at the plot with age on the x-axis (or exploring Hadley Wickham&amp;rsquo;s NEISS Shiny app) but the NEISS data set includes quite a bit more demographic information to interact with.</description>
    </item>
    
    <item>
      <title>Fatal Police Shootings Across the U.S.</title>
      <link>/blog/fatal-shootings/</link>
      <pubDate>Thu, 07 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/fatal-shootings/</guid>
      <description>I have been full of grief and sadness and some anger in the wake of yet more videos going viral in the past couple days showing black men being killed by police officers. I am not an expert on what it means to be a person of color in the United States or what is or isn&amp;rsquo;t wrong with policing today here, but it sure feels like something is deeply broken.</description>
    </item>
    
    <item>
      <title>Term Frequency and tf-idf Using Tidy Data Principles</title>
      <link>/blog/term-frequency-tf-idf/</link>
      <pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/term-frequency-tf-idf/</guid>
      <description>At the end of last week, Dave Robinson and I released a new version of tidytext on CRAN, our R package for text mining using tidy data principles. You can check out my first blog post about tidytext to learn a bit about the philosophy of the package and see some of the ways to use it, or see the package on GitHub. In this new release (tidytext 0.1.1), we have added more documentation, fixed some bugs, developed better testing/CI, and added some new functionality.</description>
    </item>
    
    <item>
      <title>A Beginner&#39;s Guide to Travis-CI for R</title>
      <link>/blog/beginners-guide-to-travis/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/beginners-guide-to-travis/</guid>
      <description>Have you seen all those attractive green badges on other people&amp;rsquo;s R packages and thought, &amp;ldquo;I want a lovely green badge!&amp;rdquo;
OF COURSE YOU DO. Well, let&amp;rsquo;s give it a shot, because today I am going to attempt a beginner&amp;rsquo;s guide to using Travis-CI for continuous integration for R packages. It is going to be a beginner&amp;rsquo;s guide because that is all I could possibly write; my knowledge and experience with Travis is limited.</description>
    </item>
    
    <item>
      <title>The Life-Changing Magic of Tidying Text</title>
      <link>/blog/life-changing-magic/</link>
      <pubDate>Fri, 29 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/life-changing-magic/</guid>
      <description>When I went to the rOpenSci unconference about a month ago, I started work with Dave Robinson on a package for text mining using tidy data principles. What is this tidy data you keep hearing so much about? As described by Hadley Wickham, tidy data has a specific structure:
 each variable is a column each observation is a row each type of observational unit is a table  This means we end up with a data set that is in a long, skinny format instead of a wide format.</description>
    </item>
    
    <item>
      <title>How I Learned to Stop Worrying and Love R CMD Check</title>
      <link>/blog/how-i-stopped/</link>
      <pubDate>Mon, 18 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/how-i-stopped/</guid>
      <description>Last week, I officially became the maintainer of a CRAN package! My package for the texts of Jane Austen&amp;rsquo;s 6 completed, published novels, janeaustenr, was released on CRAN and my Twitter feed was filled with congratulatory Jane Austen GIFs. I think this might be my favorite.
It was a good day.
During the process of getting janeaustenr ready to submit to CRAN, I was pointed to some resources that were very helpful to me as a first-time maintainer.</description>
    </item>
    
    <item>
      <title>Who Came to Vote in Utah&#39;s Caucuses?</title>
      <link>/blog/who-came-to-vote/</link>
      <pubDate>Fri, 08 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/who-came-to-vote/</guid>
      <description>Late last month, I analyzed results from Utah&amp;rsquo;s Republican and Democratic caucuses to show how the different presidential candidates fared across Utah. That was fun work to do, but I realized there was one more map I wanted to make; I want to compare the Republican and Democratic voter turnout across the counties in Utah. Utah is a politically conservative state and we know from the last plot I made in that post that many more people voted in the Republican caucus than the Democratic caucus, but I would like to see how voter turnout was distributed across the state.</description>
    </item>
    
    <item>
      <title>I Went to ROpenSci Unconference and All I Got Were These Lousy Hex Stickers</title>
      <link>/blog/i-went-to-ropensci/</link>
      <pubDate>Wed, 06 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/i-went-to-ropensci/</guid>
      <description>Just kidding; it was amazing.
Last week, I traveled to San Francisco to participate in an unconference/hackathon organized and hosted by ROpenSci. This was my first R conference or meeting, and it was a such a great experience. I am still feeling a bit at a loss for words about what a tremendous time I had, actually, but I will make an attempt to share a bit about what it was like and what we did.</description>
    </item>
    
    <item>
      <title>Trump Losing and Feeling the Bern in Utah</title>
      <link>/blog/mapping-utah-caucus/</link>
      <pubDate>Fri, 25 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/mapping-utah-caucus/</guid>
      <description>Well, it&amp;rsquo;s been an interesting election season so far, right? Everybody holding up OK? Utah held its caucuses this past Tuesday on March 22 and I thought I would do a bit of plotting to show the results. We can get the JSON data from CNN, as pointed out by Bob Rudis in his post here. Utah&amp;rsquo;s results were not available when he wrote that post but I was able to poke around and find them using the guidance he provided there.</description>
    </item>
    
    <item>
      <title>If I Loved Natural Language Processing Less, I Might Be Able to Talk About It More</title>
      <link>/blog/if-i-loved-nlp-less/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/if-i-loved-nlp-less/</guid>
      <description>In my last post, I did some natural language processing and sentiment analysis for Jane Austen&amp;rsquo;s most well-known novel, Pride and Prejudice. It was just so much fun that I wanted to extend some of that work and compare across her body of writing.
I decided to make an R package for her texts, for easy access for myself and anybody else who would like to do some text analysis on a nice sample of prose.</description>
    </item>
    
    <item>
      <title>You Must Allow Me To Tell You How Ardently I Admire and Love Natural Language Processing</title>
      <link>/blog/you-must-allow-me/</link>
      <pubDate>Tue, 08 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/you-must-allow-me/</guid>
      <description>It is a truth universally acknowledged that sentiment analysis is super fun, and Pride and Prejudice is probably my very favorite book in all of literature, so let&amp;rsquo;s do some Jane Austen natural language processing.
Project Gutenberg makes e-texts available for many, many books, including Pride and Prejudice which is available here. I am using the plain text UTF-8 file available at that link for this analysis. Let&amp;rsquo;s read the file and get it ready for analysis.</description>
    </item>
    
    <item>
      <title>My Baby Boomer Name Might Have Been &#34;Debbie&#34;</title>
      <link>/blog/my-baby-boomer-name/</link>
      <pubDate>Mon, 29 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/my-baby-boomer-name/</guid>
      <description>I have always loved learning and thinking about names, how they are chosen and used, and how people feel about their names and the names around them. We had a traditional baby name book at our house when I was growing up (you know, lists of names with meanings), and I remember poring over it to find unusual or appealing names for my pretend play or the stories I wrote. As an adult, I read Laura Wattenberg&amp;rsquo;s excellent book on baby names when we were expecting our second baby, and I also discovered the NameVoyager on Wattenberg&amp;rsquo;s website.</description>
    </item>
    
    <item>
      <title>Your Floor Is the Most Dangerous Thing In Your House</title>
      <link>/blog/your-floor/</link>
      <pubDate>Wed, 17 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/your-floor/</guid>
      <description>I saw this analysis at Flowing Data about the most common consumer products involved in hospital ER visits and was delighted, interested, etc. Nathan&amp;rsquo;s next related post is, um, also super interesting, if entirely horrifying. Apparently, I am not the only one who thought this data set was compelling, because this week Hadley Wickham took the NEISS data set that these beautiful analyses are based on and made an R package for them.</description>
    </item>
    
    <item>
      <title>A Tall Drink of Water</title>
      <link>/blog/tall-drink-of-water/</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/tall-drink-of-water/</guid>
      <description>In a previous post, I used water consumption data from Utah&amp;rsquo;s Open Data Catalog to explore what kind of users consume the most water in my home here in Salt Lake City, what the annual pattern of water use is, and how the drought of the past few years has affected water use. I made a predictive model for the total aggregate water use of the city and tested how drought affected the accuracy of such a model.</description>
    </item>
    
    <item>
      <title>Death Comes to Us All</title>
      <link>/blog/death-comes/</link>
      <pubDate>Fri, 05 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/death-comes/</guid>
      <description>I have been working with a data set on causes of death in my adopted home state of Utah for a little while now, and I had been struggling with the best way to visualize it. This week, David Robinson released the gganimate package to create animated ggplot2 plots and I thought &amp;ldquo;AH HA! This is what I have needing.&amp;rdquo; The data on causes of death in Utah is available here via Utah&amp;rsquo;s Open Data Catalog and can be accessed via Socrata Open Data API.</description>
    </item>
    
    <item>
      <title>Connecting Religion and Demographics</title>
      <link>/blog/connecting-religion/</link>
      <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/connecting-religion/</guid>
      <description>I have my second guest post up today at Ari Lamstein&amp;rsquo;s blog where I conclude my exploration of the Religious Congregations and Membership Study at the ARDA. In this post I show how we can look at the relationships between a data set like the religion census and demographic data to gain context and understanding. Go over there to read the details!</description>
    </item>
    
    <item>
      <title>More Fun with Choropleth Maps</title>
      <link>/blog/more-fun-with-maps/</link>
      <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/more-fun-with-maps/</guid>
      <description>I have a guest post up today at Ari Lamstein&amp;rsquo;s blog where I show some more fun things that can be done with the Religious Congregations and Membership Study at the ARDA that I used to look at Utah. I looked in some detail at Iowa ahead of their caucus in a few days, in light of all the news lately about Republican presidential candidates courting evangelical voters. Go take a look to read more!</description>
    </item>
    
    <item>
      <title>Water World</title>
      <link>/blog/water-world/</link>
      <pubDate>Tue, 19 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/water-world/</guid>
      <description>I live in Utah, an extremely dry state. Like much of the western United States, Utah is experiencing water stress from increasing demand, episodes of drought, and conflict over water rights. At the same time, Utahns use a lot of water per capita compared to residents of other states. According to the United States Geological Survey, in 2014 people in Utah used more water per person than in any other state, and in years before and after, Utah‚Äôs per capita water use is always near the very top in the U.</description>
    </item>
    
    <item>
      <title>Health Care Indicators in Utah Counties</title>
      <link>/blog/health-care-indicators/</link>
      <pubDate>Mon, 11 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/health-care-indicators/</guid>
      <description>The state of Utah (my adopted home) has an Open Data Catalog with lots of interesting data sets, including a collection of health care indicators from 2014 for the 29 counties in Utah. The observations for each county include measurements such as the infant mortality rate, the percent of people who don&amp;rsquo;t have insurance, what percent of people have diabetes, and so forth. Let&amp;rsquo;s see how these health care indicators are related to each other and if we can use these data to cluster Utah counties into similar groups.</description>
    </item>
    
    <item>
      <title>This Is the Place, Apparently</title>
      <link>/blog/this-is-the-place/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/this-is-the-place/</guid>
      <description>My family and I moved to Utah about 5 years ago and we have found ourselves thoroughly in love in with our new home state. I didn&amp;rsquo;t know much about it before we began the process of contemplating a move here, and I find that is often true of many people. Let&amp;rsquo;s use some choropleth maps and demographic exploration to learn a bit more about this place I call home now.</description>
    </item>
    
    <item>
      <title>Joy to the World, and also Anticipation, Disgust, Surprise...</title>
      <link>/blog/joy-to-the-world/</link>
      <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/joy-to-the-world/</guid>
      <description>In my previous blog post, I analyzed my Twitter archive and explored some aspects of my tweeting behavior. When do I tweet, how much do retweet people, do I use hashtags? These are examples of one kind of question, but what about the actual verbal content of my tweets, the text itself? What kinds of questions can we ask and answer about the text in some programmatic way? This is what is called natural language processing, and I&amp;rsquo;ll give a first shot at it here.</description>
    </item>
    
    <item>
      <title>Ten Thousand Tweets</title>
      <link>/blog/ten-thousand-tweets/</link>
      <pubDate>Tue, 08 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/ten-thousand-tweets/</guid>
      <description>I started learning the statistical programming language R this past summer, and discovering Hadley Wickham&amp;rsquo;s data visualization package ggplot2 has been a joy and a revelation. When I think back to how I made all the plots for my astronomy dissertation in the early 2000s (COUGH SUPERMONGO COUGH), I feel a bit in awe of what ggplot2 can do and how easy and, might I even say, delightful it is to use.</description>
    </item>
    
  </channel>
</rss>